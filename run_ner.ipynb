{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gerry.xu/miniforge3/envs/paddle_env/lib/python3.9/site-packages/paddlenlp/transformers/image_utils.py:213: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  resample=Image.BILINEAR,\n",
      "/Users/gerry.xu/miniforge3/envs/paddle_env/lib/python3.9/site-packages/paddlenlp/transformers/image_utils.py:379: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  resample=Image.NEAREST,\n",
      "/Users/gerry.xu/miniforge3/envs/paddle_env/lib/python3.9/site-packages/paddlenlp/transformers/ernie_vil/feature_extraction.py:65: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  resample=Image.BICUBIC,\n",
      "/Users/gerry.xu/miniforge3/envs/paddle_env/lib/python3.9/site-packages/paddlenlp/transformers/clip/feature_extraction.py:64: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  resample=Image.BICUBIC,\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import paddle\n",
    "from paddle.io import DataLoader, BatchSampler\n",
    "from paddlenlp.data import DataCollatorForTokenClassification\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.metrics import ChunkEvaluator\n",
    "from paddlenlp.transformers import AutoModelForTokenClassification\n",
    "from paddlenlp.transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# data arguments\n",
    "train_file_path = \"datasets/msra_ner/train.tsv\"\n",
    "test_file_path = \"datasets/msra_ner/test.tsv\"\n",
    "label_map_file_path = \"datasets/msra_ner/label_map.json\"\n",
    "max_seq_length = 128\n",
    "\n",
    "# model arguments\n",
    "model_name = \"ernie-3.0-medium-zh\"\n",
    "\n",
    "# paddle training arguments\n",
    "batch_size = 32\n",
    "learning_rate = 2e-5\n",
    "epochs = 10\n",
    "ckpt_dir = \"ernie_ckpt/ernie-3.0-medium-zh-msra-ner\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def read(data_path):\n",
    "    with open(data_path, \"r\") as fin:\n",
    "        for line in fin:\n",
    "            line = line.rstrip()\n",
    "            tokens_str, labels_str = line.split(\"\\t\")\n",
    "            tokens = tokens_str.split(\"\\002\")\n",
    "            labels = labels_str.split(\"\\002\")\n",
    "            yield {\"tokens\": tokens, \"labels\": labels}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# train_ds, test_ds = load_dataset('msra_ner', splits=('train', 'test'), lazy=False)\n",
    "\n",
    "train_ds = load_dataset(read, data_path=train_file_path, lazy=False)\n",
    "test_ds = load_dataset(read, data_path=test_file_path, lazy=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# all_labels = train_ds.label_list\n",
    "# label_2_label_id = {label: i for i, label in enumerate(all_labels)}\n",
    "# label_id_2_label = {label_id: label for label, label_id in label_2_label_id.items()}\n",
    "# num_classes = len(all_labels)\n",
    "\n",
    "label_map = json.load(open(label_map_file_path, \"r\"))\n",
    "num_classes = len(label_map)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33m[2022-09-28 14:43:52,908] [ WARNING]\u001B[0m - Can't find the faster_tokenizer package, please ensure install faster_tokenizer correctly. You can install faster_tokenizer by `pip install faster_tokenizer`.\u001B[0m\n",
      "\u001B[32m[2022-09-28 14:43:52,908] [    INFO]\u001B[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'ernie-3.0-medium-zh'.\u001B[0m\n",
      "\u001B[32m[2022-09-28 14:43:52,909] [    INFO]\u001B[0m - Already cached /Users/gerry.xu/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt\u001B[0m\n",
      "\u001B[32m[2022-09-28 14:43:52,925] [    INFO]\u001B[0m - tokenizer config file saved in /Users/gerry.xu/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json\u001B[0m\n",
      "\u001B[32m[2022-09-28 14:43:52,926] [    INFO]\u001B[0m - Special tokens file saved in /Users/gerry.xu/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json\u001B[0m\n",
      "\u001B[32m[2022-09-28 14:43:52,927] [    INFO]\u001B[0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification'> to load 'ernie-3.0-medium-zh'.\u001B[0m\n",
      "\u001B[32m[2022-09-28 14:43:52,928] [    INFO]\u001B[0m - Already cached /Users/gerry.xu/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh.pdparams\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_faster=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_classes=num_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def _preprocess(example, tokenizer, label_map, max_seq_length=128):\n",
    "    tokens = example[\"tokens\"]  # list of tokens\n",
    "    labels = [label_map[label] for label in example[\"labels\"]]  # list of label ids\n",
    "    no_entity_id = label_map[\"O\"]\n",
    "\n",
    "    tokens_encoded = tokenizer(tokens, return_length=True, is_split_into_words=True, max_seq_len=max_seq_length)\n",
    "\n",
    "    input_ids_len = len(tokens_encoded[\"input_ids\"])  # input_ids_len = max_seq_len\n",
    "    # 如果 input_ids_len - 2 < len(labels)，说明输入的 tokens 的长度超过 max_seq_len，被截断了\n",
    "    if input_ids_len - 2 < len(labels):\n",
    "        labels = labels[:input_ids_len - 2]\n",
    "    tokens_encoded[\"labels\"] = [no_entity_id] + labels + [no_entity_id]\n",
    "    tokens_encoded[\"labels\"] += [no_entity_id] * (input_ids_len - len(tokens_encoded[\"labels\"]))\n",
    "\n",
    "    return tokens_encoded"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "trans_func = functools.partial(_preprocess, tokenizer=tokenizer, label_map=label_map, max_seq_length=max_seq_length)\n",
    "train_ds = train_ds.map(trans_func)\n",
    "test_ds = test_ds.map(trans_func)\n",
    "\n",
    "# collate_fn 函数将不同长度序列补齐到批中数据的最大长度，再将数据堆叠\n",
    "collate_fn = DataCollatorForTokenClassification(tokenizer=tokenizer, label_pad_token_id=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_batch_sampler = BatchSampler(train_ds, batch_size=batch_size, shuffle=True)\n",
    "train_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, collate_fn=collate_fn)\n",
    "\n",
    "test_batch_sampler = BatchSampler(test_ds, batch_size=batch_size, shuffle=False)\n",
    "test_data_loader = DataLoader(dataset=test_ds, batch_sampler=test_batch_sampler, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# AdamW 优化器、交叉熵损失函数、ChunkEvaluator 评价指标\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=learning_rate, parameters=model.parameters())\n",
    "loss_obj = paddle.nn.loss.CrossEntropyLoss(ignore_index=-1)\n",
    "metric_obj = ChunkEvaluator(label_list=list(label_map.keys()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "@paddle.no_grad()\n",
    "def evaluate(data_loader, model, metric):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "\n",
    "    precision, recall, f1_score = 0, 0, 0\n",
    "    for step, batch in enumerate(data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels, lens = batch['input_ids'], batch['token_type_ids'], batch['labels'], batch['seq_len']\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        preds = paddle.argmax(logits, axis=-1)\n",
    "        n_infer, n_label, n_correct = metric.compute(lens, preds, labels)\n",
    "        metric.update(n_infer.numpy(), n_label.numpy(), n_correct.numpy())\n",
    "        precision, recall, f1_score = metric.accumulate()\n",
    "\n",
    "    print(\"eval precision: %.6f - recall: %.6f - f1: %.6f\" % (precision, recall, f1_score))\n",
    "    model.train()\n",
    "    return precision, recall, f1_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     21\u001B[0m     tic_train \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# 反向梯度回传\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     26\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mclear_grad()\n",
      "File \u001B[0;32m~/miniforge3/envs/paddle_env/lib/python3.9/site-packages/decorator.py:232\u001B[0m, in \u001B[0;36mdecorate.<locals>.fun\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwsyntax:\n\u001B[1;32m    231\u001B[0m     args, kw \u001B[38;5;241m=\u001B[39m fix(args, kw, sig)\n\u001B[0;32m--> 232\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcaller\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mextras\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/paddle_env/lib/python3.9/site-packages/paddle/fluid/wrapped_decorator.py:25\u001B[0m, in \u001B[0;36mwrap_decorator.<locals>.__impl__\u001B[0;34m(func, *args, **kwargs)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;129m@decorator\u001B[39m\u001B[38;5;241m.\u001B[39mdecorator\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__impl__\u001B[39m(func, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     24\u001B[0m     wrapped_func \u001B[38;5;241m=\u001B[39m decorator_func(func)\n\u001B[0;32m---> 25\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/paddle_env/lib/python3.9/site-packages/paddle/fluid/framework.py:434\u001B[0m, in \u001B[0;36m_dygraph_only_.<locals>.__impl__\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    431\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__impl__\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    432\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m _non_static_mode(\n\u001B[1;32m    433\u001B[0m     ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWe only support \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m()\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m in dynamic graph mode, please call \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpaddle.disable_static()\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m to enter dynamic graph mode.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\n\u001B[0;32m--> 434\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/paddle_env/lib/python3.9/site-packages/paddle/fluid/dygraph/varbase_patch_methods.py:290\u001B[0m, in \u001B[0;36mmonkey_patch_varbase.<locals>.backward\u001B[0;34m(self, grad_tensor, retain_graph)\u001B[0m\n\u001B[1;32m    288\u001B[0m         core\u001B[38;5;241m.\u001B[39meager\u001B[38;5;241m.\u001B[39mrun_backward([\u001B[38;5;28mself\u001B[39m], grad_tensor, retain_graph)\n\u001B[1;32m    289\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 290\u001B[0m         \u001B[43mcore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdygraph_run_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mgrad_tensor\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    291\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    292\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43mframework\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dygraph_tracer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m in_profiler_mode():\n\u001B[1;32m    294\u001B[0m     record_event\u001B[38;5;241m.\u001B[39mend()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "global_step = 0  # 全局迭代次数\n",
    "best_step = 0\n",
    "best_f1_score = 0\n",
    "\n",
    "tic_train = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "\n",
    "        # 计算模型输出、损失函数值\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = paddle.mean(loss_obj(logits, labels))\n",
    "\n",
    "        # 每迭代 10 次，打印损失函数值、计算速度\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.6f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, 10 / (time.time() - tic_train))\n",
    "            )\n",
    "            tic_train = time.time()\n",
    "\n",
    "        # 反向梯度回传\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        # 每迭代 200 次，评估当前训练的模型、保存当前最佳模型参数和分词器的词表等\n",
    "        if global_step % 200 == 0:\n",
    "            save_dir = ckpt_dir\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            print('global_step', global_step, end=' ')\n",
    "            _, _, eval_f1_score = evaluate(test_data_loader, model, metric_obj)\n",
    "            if eval_f1_score > best_f1_score:\n",
    "                best_f1_score = eval_f1_score\n",
    "                best_step = global_step\n",
    "\n",
    "                model.save_pretrained(save_dir)\n",
    "                tokenizer.save_pretrained(save_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}